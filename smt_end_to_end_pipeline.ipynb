{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eafb0b",
   "metadata": {},
   "source": [
    "# SMT End-to-End Pipeline\n",
    "\n",
    "This notebook covers:\n",
    "- Optional web crawling (for assignment evidence)\n",
    "- Dataset loading (JSON/CSV/TSV/TXT)\n",
    "- Text cleaning + tokenization\n",
    "- Statistical Machine Translation training (IBM1-style)\n",
    "- Saving training progress and checkpoints\n",
    "- Resume training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, json, re, random, time, math, csv, shutil  # === NEW: shutil for snapshots\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Optional installs ----------\n",
    "try:\n",
    "    import jieba\n",
    "except ImportError:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"jieba\"])\n",
    "    import jieba\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords as nltk_stopwords\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\")\n",
    "\n",
    "# --- NEW: tqdm for progress bars ---\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "# Point this to your dataset (CN \\t EN per line for .txt)\n",
    "DATASET_PATH = \"../dataset_CN_EN.txt\"\n",
    "# If you use CSV/TSV/JSON with headers, set these columns accordingly.\n",
    "DATASET_TEXT_COLUMNS = [\"chinese\", \"english\"]\n",
    "\n",
    "RUN_DIR = \"./smt_runs/zh_en_pbsmt_s2t_only\"\n",
    "\n",
    "# --- NEW: Save-best/snapshot config ---\n",
    "SAVE_BEST   = True\n",
    "DEV_RATIO   = 0.1                    # 10% dev\n",
    "BEST_DIR    = os.path.join(RUN_DIR, \"best\")\n",
    "os.makedirs(BEST_DIR, exist_ok=True)\n",
    "\n",
    "# Data / training caps\n",
    "MAX_SENTENCES = 30000\n",
    "MAX_SENT_LEN = 30  # in words/tokens\n",
    "SEED = 62\n",
    "\n",
    "# IBM1\n",
    "IBM1_ITERS = 10\n",
    "IGNORE_STOPWORDS = False\n",
    "USE_IDF_WEIGHT = True\n",
    "ADD_NULL = True\n",
    "DICE_TOPK_PER_TOKEN = 40\n",
    "DICE_MIN_THRESH = 0.005\n",
    "\n",
    "# Phrase extraction\n",
    "MAX_SRC_PHRASE_LEN = 8\n",
    "PHRASE_TOPK_PER_SRC = 80\n",
    "\n",
    "# LM (stupid backoff)\n",
    "LM_ALPHA = 0.3\n",
    "\n",
    "# Decoder (with limited reordering)\n",
    "W_PHRASE = 1.0\n",
    "W_LEX = 1.0\n",
    "W_LM = 1.0\n",
    "W_WORD_PENALTY = -0.2\n",
    "MAX_JUMP = 3\n",
    "DIST_PENALTY = -0.3\n",
    "\n",
    "# Eval\n",
    "BLEU_SAMPLE_SIZE = 1000\n",
    "\n",
    "# Checkpoints\n",
    "RESET_CHECKPOINTS = False\n",
    "STATUS_PATH = os.path.join(RUN_DIR, \"status.json\")\n",
    "\n",
    "random.seed(SEED)\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(RUN_DIR, \"checkpoints\"), exist_ok=True)\n",
    "jieba.del_word(\"今天天气\")\n",
    "\n",
    "if RESET_CHECKPOINTS:\n",
    "    ckdir = os.path.join(RUN_DIR, \"checkpoints\")\n",
    "    for fn in os.listdir(ckdir):\n",
    "        try:\n",
    "            os.remove(os.path.join(ckdir, fn))\n",
    "        except:\n",
    "            pass\n",
    "    for fn in [\"status.json\", \"metrics.csv\", \"phrase_table.json\", \"lm_trigram_counts.json\"]:\n",
    "        fpath = os.path.join(RUN_DIR, fn)\n",
    "        if os.path.exists(fpath):\n",
    "            try:\n",
    "                os.remove(fpath)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "# ===========\n",
    "# Overrides (tiny curated dictionary to pin obvious phrases)\n",
    "# ===========\n",
    "OVERRIDES = {\n",
    "    (\"你好\",): (\"hello\",),\n",
    "    (\"世界\",): (\"world\",),\n",
    "    (\"你好\", \"世界\"): (\"hello\", \"world\"),\n",
    "    (\"谢谢\",): (\"thanks\",),\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# Helpers\n",
    "# ==============================\n",
    "EN_STOP = set(nltk_stopwords.words(\"english\"))\n",
    "\n",
    "# seed jieba with common MT-ish words\n",
    "for w in [\"你好\",\"谢谢\",\"世界\",\"中国\",\"我们\",\"学校\",\"喜欢\",\"学习\",\"英文\",\"中文\"]:\n",
    "    jieba.add_word(w)\n",
    "\n",
    "def tok_zh_words(s: str) -> List[str]:\n",
    "    return [t for t in jieba.lcut(str(s)) if t.strip()]\n",
    "\n",
    "def tok_en_words(s: str) -> List[str]:\n",
    "    toks = re.findall(r\"\\b\\w+\\b\", str(s).lower())\n",
    "    return [t for t in toks if (not IGNORE_STOPWORDS or t not in EN_STOP)]\n",
    "\n",
    "def _smart_txt_split(line: str) -> List[str]:\n",
    "    for sep in [\"\\t\", \" | \", \"||\", \":::\", \"|\", \",\", \" \"]:\n",
    "        if sep in line:\n",
    "            parts = line.split(sep, 1)\n",
    "            if len(parts) >= 2:\n",
    "                return [parts[0], parts[1]]\n",
    "    parts = line.strip().split(None, 1)\n",
    "    if len(parts) == 2:\n",
    "        return parts\n",
    "    return []\n",
    "\n",
    "def load_dataset(path: str, text_cols: List[str]) -> Tuple[List[str], List[str]]:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext in (\".json\", \".jsonl\"):\n",
    "        try:\n",
    "            df = pd.read_json(path, lines=True)\n",
    "        except ValueError:\n",
    "            df = pd.read_json(path)\n",
    "        if not all(c in df.columns for c in text_cols):\n",
    "            raise ValueError(f\"Columns {text_cols} not found. Available: {list(df.columns)}\")\n",
    "        df = df[text_cols].dropna().head(MAX_SENTENCES).reset_index(drop=True)\n",
    "        return list(df[text_cols[0]]), list(df[text_cols[1]])\n",
    "    elif ext in (\".csv\", \".tsv\"):\n",
    "        sep = \",\" if ext == \".csv\" else \"\\t\"\n",
    "        df = pd.read_csv(path, sep=sep)\n",
    "        if not all(c in df.columns for c in text_cols):\n",
    "            raise ValueError(f\"Columns {text_cols} not found. Available: {list(df.columns)}\")\n",
    "        df = df[text_cols].dropna().head(MAX_SENTENCES).reset_index(drop=True)\n",
    "        return list(df[text_cols[0]]), list(df[text_cols[1]])\n",
    "    else:\n",
    "        srcs, tgts = [], []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line: continue\n",
    "                parts = _smart_txt_split(line)\n",
    "                if len(parts) >= 2:\n",
    "                    srcs.append(parts[0].strip())\n",
    "                    tgts.append(parts[1].strip())\n",
    "                if len(srcs) >= MAX_SENTENCES:\n",
    "                    break\n",
    "        if not srcs:\n",
    "            raise ValueError(\"Could not parse any lines from .txt.\")\n",
    "        return srcs, tgts\n",
    "\n",
    "def bleu_corpus(refs: List[List[str]], hyps: List[List[str]], max_n=4) -> float:\n",
    "    def ngrams(seq, n):\n",
    "        return [tuple(seq[i:i+n]) for i in range(len(seq)-n+1)]\n",
    "    logs = []\n",
    "    for n in range(1, max_n+1):\n",
    "        match = total = 0\n",
    "        for r, h in zip(refs, hyps):\n",
    "            rc, hc = Counter(ngrams(r, n)), Counter(ngrams(h, n))\n",
    "            total += sum(hc.values())\n",
    "            for g, c in hc.items():\n",
    "                match += min(c, rc.get(g, 0))\n",
    "        logs.append(float(\"-inf\") if total == 0 or match == 0 else math.log(match/total))\n",
    "    ref_len = sum(len(r) for r in refs)\n",
    "    hyp_len = sum(len(h) for h in hyps)\n",
    "    bp = 1.0 if hyp_len > ref_len else math.exp(1 - ref_len/max(hyp_len,1))\n",
    "    gm = 0.0 if any(x == float(\"-inf\") for x in logs) else math.exp(sum(logs)/len(logs))\n",
    "    return bp * gm\n",
    "\n",
    "def append_metrics(row: Dict[str, str]):\n",
    "    path = os.path.join(RUN_DIR, \"metrics.csv\")\n",
    "    write_header = not os.path.exists(path)\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=sorted(row.keys()))\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "def update_status(stage: str, it: int, total_iters: int, last_ckpt: str, extra: dict = None):\n",
    "    try:\n",
    "        status = {}\n",
    "        if os.path.exists(STATUS_PATH):\n",
    "            with open(STATUS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "                status = json.load(f)\n",
    "        status[stage] = {\n",
    "            \"current_iter\": it,\n",
    "            \"total_iters\": total_iters,\n",
    "            \"last_checkpoint\": last_ckpt,\n",
    "            \"updated_at\": _now_iso()\n",
    "        }\n",
    "        if extra:\n",
    "            status[stage].update(extra)\n",
    "        with open(STATUS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(status, f, ensure_ascii=False, indent=2)\n",
    "    except Exception as e:\n",
    "        print(\"[status] WARN:\", e)\n",
    "\n",
    "# ==============================\n",
    "# Prepare data\n",
    "# ==============================\n",
    "src_raw, tgt_raw = load_dataset(DATASET_PATH, DATASET_TEXT_COLUMNS)\n",
    "print(\"Loaded\", len(src_raw), \"pairs\")\n",
    "\n",
    "pairs = []\n",
    "for z, e in tqdm(zip(src_raw, tgt_raw), total=min(len(src_raw), len(tgt_raw)), desc=\"Filtering pairs\", ncols=100):\n",
    "    sw, tw = tok_zh_words(z), tok_en_words(e)\n",
    "    if 0 < len(sw) <= MAX_SENT_LEN and 0 < len(tw) <= MAX_SENT_LEN:\n",
    "        pairs.append((sw, tw))\n",
    "random.shuffle(pairs)\n",
    "print(\"After filters:\", len(pairs))\n",
    "\n",
    "# === NEW: split train/dev ===\n",
    "if SAVE_BEST:\n",
    "    n_dev = max(1, int(len(pairs) * DEV_RATIO))\n",
    "    dev_pairs = pairs[:n_dev]\n",
    "    train_pairs = pairs[n_dev:]\n",
    "    print(f\"[split] train={len(train_pairs)}  dev={len(dev_pairs)}\")\n",
    "else:\n",
    "    train_pairs = pairs\n",
    "    dev_pairs = []\n",
    "\n",
    "# Build Dice candidate pruning (on train only)\n",
    "src_df, tgt_df = Counter(), Counter()\n",
    "pair_df = defaultdict(Counter)\n",
    "for s_words, t_words in tqdm(train_pairs, desc=\"DF counting (Dice)\", ncols=100):\n",
    "    s_set, t_set = set(s_words), set(t_words)\n",
    "    for s in s_set: src_df[s] += 1\n",
    "    for t in t_set: tgt_df[t] += 1\n",
    "    for s in s_set:\n",
    "        for t in t_set:\n",
    "            pair_df[s][t] += 1\n",
    "\n",
    "candidates_s2t: Dict[str, Set[str]] = {}\n",
    "for s, t_counts in tqdm(pair_df.items(), desc=\"Dice pruning\", ncols=100):\n",
    "    scored = []\n",
    "    for t, freq in t_counts.items():\n",
    "        dice = 2.0 * freq / (src_df[s] + tgt_df[t])\n",
    "        if dice >= DICE_MIN_THRESH:\n",
    "            scored.append((t, dice))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    candidates_s2t[s] = set(t for t, _ in scored[:DICE_TOPK_PER_TOKEN])\n",
    "\n",
    "# IDF for EN (target) — on train only\n",
    "def build_idf_on(tokens_list: List[List[str]]) -> Dict[str, float]:\n",
    "    df_c = Counter()\n",
    "    for toks in tqdm(tokens_list, desc=\"IDF df count\", ncols=100):\n",
    "        df_c.update(set(toks))\n",
    "    N = len(tokens_list)\n",
    "    idf = defaultdict(lambda: 1.0)\n",
    "    for w, c in df_c.items():\n",
    "        idf[w] = math.log(1 + (N / (1 + c))) + 1.0\n",
    "    return idf\n",
    "\n",
    "idf_en = build_idf_on([t for _, t in train_pairs])\n",
    "\n",
    "# ==============================\n",
    "# IBM1 Training (s -> t only) with tqdm\n",
    "# ==============================\n",
    "def train_ibm1_s2t(pairs: List[Tuple[List[str], List[str]]],\n",
    "                   candidates: Dict[str, Set[str]],\n",
    "                   iters: int,\n",
    "                   idf_target: Dict[str, float],\n",
    "                   add_null=True) -> Dict[str, Dict[str, float]]:\n",
    "    ckdir = os.path.join(RUN_DIR, \"checkpoints\")\n",
    "    os.makedirs(ckdir, exist_ok=True)\n",
    "\n",
    "    tprobs = defaultdict(lambda: defaultdict(lambda: 1.0))\n",
    "    start_it = 1\n",
    "\n",
    "    resumes = sorted([n for n in os.listdir(ckdir) if n.startswith(\"ibm1_s2t_iter_\")])\n",
    "    if resumes:\n",
    "        last = resumes[-1]\n",
    "        tprobs = defaultdict(lambda: defaultdict(float))\n",
    "        data = load_json(os.path.join(ckdir, last))\n",
    "        for s, d in data.items():\n",
    "            for t, p in d.items():\n",
    "                tprobs[s][t] = float(p)\n",
    "        start_it = int(re.findall(r\"(\\d+)\", last)[-1]) + 1\n",
    "        print(f\"[IBM1 s2t] Resuming from {last}\")\n",
    "        update_status(\"ibm1_s2t\", start_it-1, iters, last_ckpt=os.path.join(ckdir, last))\n",
    "\n",
    "    NULL = \"<NULL>\"\n",
    "\n",
    "    # Seed OVERRIDES\n",
    "    for f_tuple, e_tuple in OVERRIDES.items():\n",
    "        if len(f_tuple) == 1 and len(e_tuple) == 1:\n",
    "            f = f_tuple[0]; e = e_tuple[0]\n",
    "            tprobs[f][e] = max(tprobs[f][e], 5.0)\n",
    "\n",
    "    # best_proxy_bleu = -1.0  # === optional: enable if you also想保存“最优 IBM1”\n",
    "    # best_ckpt_path  = None\n",
    "\n",
    "    with tqdm(total=iters, initial=start_it-1, desc=\"IBM1 s2t training (iters)\", ncols=100) as pbar:\n",
    "        for it in range(start_it, iters+1):\n",
    "            t0 = time.time()\n",
    "\n",
    "            count = defaultdict(Counter)\n",
    "            total = defaultdict(float)\n",
    "\n",
    "            for s_words, t_words in tqdm(pairs, desc=f\"EM E-step [{it}/{iters}]\", ncols=100, leave=False):\n",
    "                t_set = set(t_words)\n",
    "                if add_null: t_set = set(t_set) | {NULL}\n",
    "                for s in s_words:\n",
    "                    cand_t = candidates.get(s, set())\n",
    "                    if add_null: cand_t = set(cand_t) | {NULL}\n",
    "                    t_valid = [t for t in t_set if t in cand_t] or ([NULL] if add_null else [])\n",
    "                    if not t_valid:\n",
    "                        continue\n",
    "                    weights = {}\n",
    "                    denom = 0.0\n",
    "                    for t in t_valid:\n",
    "                        w = tprobs[s][t]\n",
    "                        if USE_IDF_WEIGHT:\n",
    "                            w *= idf_target[t] if t != NULL else 1.0\n",
    "                        weights[t] = w\n",
    "                        denom += w\n",
    "                    if denom == 0.0:\n",
    "                        eq = 1.0 / len(t_valid)\n",
    "                        for t in t_valid:\n",
    "                            weights[t] = eq\n",
    "                        denom = 1.0\n",
    "                    inv = 1.0 / denom\n",
    "                    for t, w in weights.items():\n",
    "                        frac = w * inv\n",
    "                        count[s][t] += frac\n",
    "                        total[s] += frac\n",
    "\n",
    "            for s in count:\n",
    "                s_total = total[s] if total[s] > 0 else 1.0\n",
    "                for t, c in count[s].items():\n",
    "                    tprobs[s][t] = c / s_total\n",
    "\n",
    "            # quick intrinsic BLEU proxy (top-1 per src token)\n",
    "            sample = pairs if len(pairs) <= BLEU_SAMPLE_SIZE else random.sample(pairs, BLEU_SAMPLE_SIZE)\n",
    "            hyps, refs = [], []\n",
    "            for s_words, t_words in sample:\n",
    "                hyp = []\n",
    "                for s in s_words:\n",
    "                    cands = tprobs.get(s, {})\n",
    "                    if cands:\n",
    "                        best = max([(tt, p) for tt, p in cands.items() if tt != NULL], key=lambda kv: kv[1], default=(None,0))\n",
    "                        if best[0]:\n",
    "                            hyp.append(best[0])\n",
    "                hyps.append(hyp); refs.append(t_words)\n",
    "            train_bleu = bleu_corpus(refs, hyps)\n",
    "\n",
    "            ckfile = os.path.join(ckdir, f\"ibm1_s2t_iter_{it:03d}.json\")\n",
    "            save_json({s: dict(d) for s, d in tprobs.items()}, ckfile)\n",
    "            update_status(\"ibm1_s2t\", it, iters, last_ckpt=ckfile, extra={\"bleu\": round(train_bleu, 6)})\n",
    "            append_metrics({\"stage\": \"ibm1_s2t\", \"iter\": it, \"bleu\": f\"{train_bleu:.6f}\"})\n",
    "\n",
    "            # if SAVE_BEST and train_bleu > best_proxy_bleu:\n",
    "            #     best_proxy_bleu = train_bleu\n",
    "            #     shutil.copyfile(ckfile, os.path.join(BEST_DIR, \"ibm1_s2t_best.json\"))\n",
    "            #     save_json({\"iter\": it, \"proxy_bleu\": float(train_bleu)}, os.path.join(BEST_DIR, \"ibm1_meta.json\"))\n",
    "\n",
    "            pbar.set_postfix_str(f\"BLEU={train_bleu*100:.2f}, {time.time()-t0:.1f}s\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    return tprobs\n",
    "\n",
    "# 使用 train_pairs 训练\n",
    "tprobs_s2t = train_ibm1_s2t(train_pairs, candidates_s2t, IBM1_ITERS, idf_en, add_null=ADD_NULL)\n",
    "save_json({s: dict(d) for s, d in tprobs_s2t.items()}, os.path.join(RUN_DIR, \"ibm1_s2t_final.json\"))\n",
    "print(\"\\n=== Training status ===\")\n",
    "print(json.dumps({\"ibm1_s2t\": {\"current_iter\": IBM1_ITERS, \"total_iters\": IBM1_ITERS,\n",
    "      \"last_checkpoint\": os.path.join(RUN_DIR, \"checkpoints\", f\"ibm1_s2t_iter_{IBM1_ITERS:03d}.json\"),\n",
    "      \"bleu\":\"(see metrics.csv)\"}}, ensure_ascii=False, indent=2))\n",
    "\n",
    "# ==============================\n",
    "# Viterbi Alignments (s2t only)\n",
    "# ==============================\n",
    "NULL = \"<NULL>\"\n",
    "NULL_PENALTY = 0.3\n",
    "\n",
    "def viterbi_align_one_s2t(s_words, t_words, tprobs):\n",
    "    aligns = set()\n",
    "    for i, s in enumerate(s_words):\n",
    "        best_j, best_p = -1, 0.0\n",
    "        for j, t in enumerate(t_words):\n",
    "            p = tprobs.get(s, {}).get(t, 0.0)\n",
    "            if p > best_p:\n",
    "                best_p, best_j = p, j\n",
    "        p_null = tprobs.get(s, {}).get(NULL, 0.0) * NULL_PENALTY\n",
    "        if p_null >= best_p:\n",
    "            continue\n",
    "        if best_j >= 0:\n",
    "            aligns.add((i, best_j))\n",
    "    return aligns\n",
    "\n",
    "alignments = []\n",
    "ck_align_path = os.path.join(RUN_DIR, \"checkpoints\", \"alignments_s2t.jsonl\")\n",
    "with open(ck_align_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for idx, (s_words, t_words) in tqdm(list(enumerate(train_pairs)), desc=\"Saving alignments\", ncols=100):\n",
    "        A = viterbi_align_one_s2t(s_words, t_words, tprobs_s2t)\n",
    "        alignments.append(A)\n",
    "        fout.write(json.dumps({\"idx\": idx, \"s\": s_words, \"t\": t_words, \"a\": sorted(list(A))}, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Saved alignments:\", ck_align_path)\n",
    "\n",
    "# ==============================\n",
    "# Phrase Extraction (Koehn-style, using s2t alignments)\n",
    "# ==============================\n",
    "def extract_phrases_for_sentence(s_words, t_words, align_set: Set[Tuple[int,int]], max_src_len=MAX_SRC_PHRASE_LEN):\n",
    "    phrases = []\n",
    "    I, J = len(s_words), len(t_words)\n",
    "    aligned_to_t = defaultdict(set)\n",
    "    aligned_to_s = defaultdict(set)\n",
    "    for i,j in align_set:\n",
    "        aligned_to_s[i].add(j)\n",
    "        aligned_to_t[j].add(i)\n",
    "    for i1 in range(I):\n",
    "        for i2 in range(i1, min(I, i1 + max_src_len)):\n",
    "            js = [j for i in range(i1, i2+1) for j in aligned_to_s.get(i, [])]\n",
    "            if not js: continue\n",
    "            j_min, j_max = min(js), max(js)\n",
    "            out = False\n",
    "            for j in range(j_min, j_max+1):\n",
    "                for i in aligned_to_t.get(j, []):\n",
    "                    if i < i1 or i > i2:\n",
    "                        out = True; break\n",
    "                if out: break\n",
    "            if out: continue\n",
    "            # expand over unaligned target words\n",
    "            j1 = j_min\n",
    "            while j1 >= 0 and (j1 not in aligned_to_t): j1 -= 1\n",
    "            j1 += 1\n",
    "            j2 = j_max\n",
    "            while j2 < J and (j2 not in aligned_to_t): j2 += 1\n",
    "            j2 -= 1\n",
    "            for y1 in range(j1, j_min+1):\n",
    "                for y2 in range(j_max, j2+1):\n",
    "                    f = tuple(s_words[i1:i2+1])\n",
    "                    e = tuple(t_words[y1:y2+1])\n",
    "                    phrases.append((f, e))\n",
    "    return phrases\n",
    "\n",
    "phrase_counts = Counter()\n",
    "src_phrase_total = Counter()\n",
    "for (s_words, t_words), align in tqdm(list(zip(train_pairs, alignments)), desc=\"Extracting phrases\", ncols=100):\n",
    "    extracted = extract_phrases_for_sentence(s_words, t_words, align, max_src_len=MAX_SRC_PHRASE_LEN)\n",
    "    for f, e in extracted:\n",
    "        phrase_counts[(f, e)] += 1\n",
    "        src_phrase_total[f] += 1\n",
    "\n",
    "# Inject OVERRIDES directly as phrases with strong mass\n",
    "for f_tuple, e_tuple in OVERRIDES.items():\n",
    "    phrase_counts[(f_tuple, e_tuple)] += 1000\n",
    "    src_phrase_total[f_tuple] += 1000\n",
    "\n",
    "# φ(e|f)\n",
    "phrase_table = defaultdict(lambda: defaultdict(float))\n",
    "for (f, e), c in phrase_counts.items():\n",
    "    phrase_table[f][e] = c / max(1, src_phrase_total[f])\n",
    "\n",
    "# ==============================\n",
    "# Lexical weights (using IBM1)\n",
    "# ==============================\n",
    "def lexical_weight_e_given_f(e: Tuple[str, ...], f: Tuple[str, ...], tprobs_s2t) -> float:\n",
    "    prod = 1.0\n",
    "    for ei in e:\n",
    "        numer = sum(tprobs_s2t.get(fj, {}).get(ei, 0.0) for fj in f)\n",
    "        denom = len(f)\n",
    "        if numer == 0.0:\n",
    "            numer = tprobs_s2t.get(\"<NULL>\", {}).get(ei, 0.0)\n",
    "            denom = 1\n",
    "        prod *= max(numer / max(denom,1), 1e-12)\n",
    "    return prod\n",
    "\n",
    "lex_table = defaultdict(lambda: defaultdict(float))\n",
    "for f, e_dict in tqdm(list(phrase_table.items()), desc=\"Lexical weights\", ncols=100):\n",
    "    for e in e_dict:\n",
    "        lex_table[f][e] = lexical_weight_e_given_f(e, f, tprobs_s2t)\n",
    "\n",
    "# Trim phrase table to top-K per source phrase (by φ * lex)\n",
    "for f, e_dict in tqdm(list(phrase_table.items()), desc=\"Trim phrase table\", ncols=100):\n",
    "    scored = []\n",
    "    for e, phi in e_dict.items():\n",
    "        score = math.log(max(phi, 1e-12)) + 0.5 * math.log(max(lex_table[f][e], 1e-12))\n",
    "        scored.append((e, score))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    keep = set([e for e, _ in scored[:PHRASE_TOPK_PER_SRC]])\n",
    "    phrase_table[f] = {e: phrase_table[f][e] for e in keep}\n",
    "    lex_table[f] = {e: lex_table[f][e] for e in keep}\n",
    "\n",
    "# Also inject singleton backoff from IBM1 (topk) — 放在保存前\n",
    "BACKOFF_TOPK = 5\n",
    "for s, d in list(tprobs_s2t.items()):\n",
    "    if s == NULL: continue\n",
    "    f = (s,)\n",
    "    ranked = sorted([(t, p) for t, p in d.items() if t != NULL], key=lambda x: x[1], reverse=True)[:BACKOFF_TOPK]\n",
    "    if not ranked: continue\n",
    "    phrase_table.setdefault(f, {})\n",
    "    lex_table.setdefault(f, {})\n",
    "    for t, p in ranked:\n",
    "        e = (t,)\n",
    "        phi = max(p, 1e-6)\n",
    "        phrase_table[f][e] = max(phrase_table[f].get(e, 0.0), phi)\n",
    "        lex_table[f][e] = max(lex_table[f].get(e, 0.0), max(p, 1e-12))\n",
    "\n",
    "# 现在再保存，确保落盘和解码一致\n",
    "save_json({\n",
    "    \"phi\": { \" \".join(f): { \" \".join(e): v for e, v in ed.items() } for f, ed in phrase_table.items() },\n",
    "    \"lex\": { \" \".join(f): { \" \".join(e): v for e, v in ed.items() } for f, ed in lex_table.items() }\n",
    "}, os.path.join(RUN_DIR, \"phrase_table.json\"))\n",
    "print(\"Saved phrase_table.json\")\n",
    "\n",
    "# ==============================\n",
    "# Simple trigram LM (stupid backoff)\n",
    "# ==============================\n",
    "BOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "def build_lm_trigram(corpus: List[List[str]]):\n",
    "    unigrams = Counter(); bigrams = Counter(); trigrams = Counter()\n",
    "    for toks in tqdm(corpus, desc=\"Build LM n-grams\", ncols=100):\n",
    "        seq = [BOS, BOS] + toks + [EOS]\n",
    "        for i in range(2, len(seq)):\n",
    "            unigrams[seq[i]] += 1\n",
    "            bigrams[(seq[i-1], seq[i])] += 1\n",
    "            trigrams[(seq[i-2], seq[i-1], seq[i])] += 1\n",
    "    total_unigrams = sum(unigrams.values())\n",
    "\n",
    "    def logprob(nextw, w1, w2):\n",
    "        tri = trigrams.get((w1, w2, nextw), 0)\n",
    "        bi = bigrams.get((w2, nextw), 0)\n",
    "        uni = unigrams.get(nextw, 0)\n",
    "        if tri > 0:\n",
    "            denom = bigrams.get((w1, w2), 1)\n",
    "            return math.log(tri / denom)\n",
    "        elif bi > 0:\n",
    "            denom = unigrams.get(w2, 1)\n",
    "            return math.log(LM_ALPHA * bi / denom)\n",
    "        else:\n",
    "            return math.log(LM_ALPHA * LM_ALPHA * (uni + 1) / (total_unigrams + len(unigrams) + 1))\n",
    "    return logprob, {\"unigrams\": unigrams,\n",
    "                     \"bigrams\": {\" \".join(k): v for k,v in bigrams.items()},\n",
    "                     \"trigrams\": {\" \".join(k): v for k,v in trigrams.items()}}\n",
    "\n",
    "lm_logprob, lm_counts = build_lm_trigram([t for _, t in train_pairs])\n",
    "save_json(lm_counts, os.path.join(RUN_DIR, \"lm_trigram_counts.json\"))\n",
    "print(\"Saved LM counts\")\n",
    "\n",
    "# ==============================\n",
    "# Phrase-based decoder with limited jumps\n",
    "# ==============================\n",
    "src_phrase_index = {}\n",
    "for f, e_dict in phrase_table.items():\n",
    "    candidates = []\n",
    "    for e, phi in e_dict.items():\n",
    "        lp = math.log(max(phi, 1e-12))\n",
    "        ll = math.log(max(lex_table[f][e], 1e-12))\n",
    "        candidates.append((e, lp, ll))\n",
    "    src_phrase_index[f] = candidates\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "def decode_with_jumps(s_words: List[str]) -> List[str]:\n",
    "    N = len(s_words)\n",
    "    span_options = defaultdict(list)\n",
    "    for i in range(N):\n",
    "        for L in range(1, min(MAX_SRC_PHRASE_LEN, N - i) + 1):\n",
    "            f = tuple(s_words[i:i+L])\n",
    "            if f in src_phrase_index:\n",
    "                span_options[(i, L)] = src_phrase_index[f]\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def search(mask: int, w1: str, w2: str):\n",
    "        if mask == (1 << N) - 1:\n",
    "            return ([], W_LM * lm_logprob(EOS, w1, w2))\n",
    "        best_hyp, best_score = [], -1e9\n",
    "        pos = 0\n",
    "        while pos < N and ((mask >> pos) & 1):\n",
    "            pos += 1\n",
    "        advanced = False\n",
    "\n",
    "        for L in range(1, min(MAX_SRC_PHRASE_LEN, N - pos) + 1):\n",
    "            if any(((mask >> k) & 1) for k in range(pos, pos+L)):\n",
    "                continue\n",
    "            if (pos, L) not in span_options:\n",
    "                continue\n",
    "            advanced = True\n",
    "            new_mask = mask | sum(1 << k for k in range(pos, pos+L))\n",
    "            for e_tokens, lp, ll in span_options[(pos, L)]:\n",
    "                lm_s = 0.0\n",
    "                ww1, ww2 = w1, w2\n",
    "                for tok in e_tokens:\n",
    "                    lm_s += lm_logprob(tok, ww1, ww2)\n",
    "                    ww1, ww2 = ww2, tok\n",
    "                sub_hyp, sub_score = search(new_mask, ww1, ww2)\n",
    "                score = sub_score + W_PHRASE*lp + W_LEX*ll + W_LM*lm_s + W_WORD_PENALTY*len(e_tokens)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_hyp = list(e_tokens) + sub_hyp\n",
    "\n",
    "        for jump in range(1, MAX_JUMP + 1):\n",
    "            jpos = pos + jump\n",
    "            if jpos >= N: break\n",
    "            if (mask >> jpos) & 1: continue\n",
    "            for L in range(1, min(MAX_SRC_PHRASE_LEN, N - jpos) + 1):\n",
    "                if any(((mask >> k) & 1) for k in range(jpos, jpos+L)):\n",
    "                    continue\n",
    "                if (jpos, L) not in span_options: continue\n",
    "                advanced = True\n",
    "                new_mask = mask | sum(1 << k for k in range(jpos, jpos+L))\n",
    "                for e_tokens, lp, ll in span_options[(jpos, L)]:\n",
    "                    lm_s = 0.0\n",
    "                    ww1, ww2 = w1, w2\n",
    "                    for tok in e_tokens:\n",
    "                        lm_s += lm_logprob(tok, ww1, ww2)\n",
    "                        ww1, ww2 = ww2, tok\n",
    "                    sub_hyp, sub_score = search(new_mask, ww1, ww2)\n",
    "                    score = sub_score + W_PHRASE*lp + W_LEX*ll + W_LM*lm_s + W_WORD_PENALTY*len(e_tokens) + DIST_PENALTY*jump\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_hyp = list(e_tokens) + sub_hyp\n",
    "\n",
    "        # backoff: word-by-word using IBM1 best\n",
    "        if not advanced:\n",
    "            s = s_words[pos]\n",
    "            cands = tprobs_s2t.get(s, {})\n",
    "            best = None\n",
    "            for t, p in sorted(cands.items(), key=lambda kv: kv[1], reverse=True):\n",
    "                if t != NULL:\n",
    "                    best = (t, p); break\n",
    "            if best:\n",
    "                tok = best[0]\n",
    "                lm_s = lm_logprob(tok, w1, w2)\n",
    "                sub_hyp, sub_score = search(mask | (1 << pos), w2, tok)\n",
    "                lp = math.log(max(best[1], 1e-12))\n",
    "                ll = lp\n",
    "                score = sub_score + W_PHRASE*lp + W_LEX*ll + W_LM*lm_s + W_WORD_PENALTY\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_hyp = [tok] + sub_hyp\n",
    "        return (best_hyp, best_score)\n",
    "\n",
    "    hyp, _ = search(0, \"<s>\", \"<s>\")\n",
    "    return hyp\n",
    "\n",
    "# ==============================\n",
    "# Save-best snapshot helpers\n",
    "# ==============================\n",
    "def save_best_snapshot(dev_bleu: float):\n",
    "    \"\"\"Save phrase table, LM counts, IBM1 and current weights into RUN_DIR/best/\"\"\"\n",
    "    if not SAVE_BEST:\n",
    "        return\n",
    "    # 1) IBM1 (prefer final; or ibm1_s2t_best if你开启了上面的best保存)\n",
    "    ibm_final = os.path.join(RUN_DIR, \"ibm1_s2t_final.json\")\n",
    "    if os.path.exists(ibm_final):\n",
    "        shutil.copyfile(ibm_final, os.path.join(BEST_DIR, \"ibm1_s2t.json\"))\n",
    "\n",
    "    # 2) 短语表 & LM\n",
    "    pt_src = os.path.join(RUN_DIR, \"phrase_table.json\")\n",
    "    lm_src = os.path.join(RUN_DIR, \"lm_trigram_counts.json\")\n",
    "    if os.path.exists(pt_src):\n",
    "        shutil.copyfile(pt_src, os.path.join(BEST_DIR, \"phrase_table.json\"))\n",
    "    if os.path.exists(lm_src):\n",
    "        shutil.copyfile(lm_src, os.path.join(BEST_DIR, \"lm_counts.json\"))\n",
    "\n",
    "    # 3) 写 meta（记录当时的解码权重与 BLEU）\n",
    "    meta = {\n",
    "        \"dev_bleu\": float(dev_bleu),\n",
    "        \"weights\": {\n",
    "            \"W_PHRASE\": W_PHRASE,\n",
    "            \"W_LEX\": W_LEX,\n",
    "            \"W_LM\": W_LM,\n",
    "            \"W_WORD_PENALTY\": W_WORD_PENALTY,\n",
    "            \"MAX_JUMP\": MAX_JUMP,\n",
    "            \"DIST_PENALTY\": DIST_PENALTY\n",
    "        }\n",
    "    }\n",
    "    save_json(meta, os.path.join(BEST_DIR, \"decode_meta.json\"))\n",
    "\n",
    "# ==============================\n",
    "# Evaluation (BLEU on dev or sample) with tqdm\n",
    "# ==============================\n",
    "def decode_bleu_on(pairs_subset):\n",
    "    hyps, refs = [], []\n",
    "    for s_words, t_words in tqdm(pairs_subset, desc=\"Decoding BLEU set\", ncols=100):\n",
    "        hyp = decode_with_jumps(s_words)\n",
    "        hyps.append(hyp); refs.append(t_words)\n",
    "    return bleu_corpus(refs, hyps)\n",
    "\n",
    "best_dev_bleu_rec_path = os.path.join(BEST_DIR, \"best_dev_bleu.json\")\n",
    "prev_best = -1.0\n",
    "if os.path.exists(best_dev_bleu_rec_path):\n",
    "    try:\n",
    "        prev_best = float(load_json(best_dev_bleu_rec_path).get(\"dev_bleu\", -1.0))\n",
    "    except:\n",
    "        prev_best = -1.0\n",
    "\n",
    "if SAVE_BEST and len(dev_pairs) > 0:\n",
    "    dev_bleu = decode_bleu_on(dev_pairs)\n",
    "    append_metrics({\"stage\": \"pbsmt_decode_dev\", \"iter\": 0, \"bleu\": f\"{dev_bleu:.6f}\"})\n",
    "    print(f\"[DEV] BLEU={dev_bleu*100:.2f} on {len(dev_pairs)} sents\")\n",
    "    if dev_bleu > prev_best:\n",
    "        print(f\"[BEST] New best dev BLEU {dev_bleu*100:.2f} > {prev_best*100:.2f}, saving snapshot…\")\n",
    "        save_best_snapshot(dev_bleu)\n",
    "        save_json({\"dev_bleu\": float(dev_bleu)}, best_dev_bleu_rec_path)\n",
    "else:\n",
    "    # 没有 dev，或不启用 SAVE_BEST 时，保留你原来的sample评估\n",
    "    sample = train_pairs if len(train_pairs) <= BLEU_SAMPLE_SIZE else random.sample(train_pairs, BLEU_SAMPLE_SIZE)\n",
    "    hyps, refs = [], []\n",
    "    t0 = time.time()\n",
    "    for s_words, t_words in tqdm(sample, desc=\"Decoding sample for BLEU\", ncols=100):\n",
    "        hyp = decode_with_jumps(s_words)\n",
    "        hyps.append(hyp); refs.append(t_words)\n",
    "    bleu = bleu_corpus(refs, hyps)\n",
    "    append_metrics({\"stage\": \"pbsmt_decode_jump_s2t_only\", \"iter\": 0, \"bleu\": f\"{bleu:.6f}\"})\n",
    "    print(f\"[PBSMT+jump s2t-only] BLEU={bleu*100:.2f} on {len(sample)} sents | {time.time()-t0:.1f}s\")\n",
    "\n",
    "# ==============================\n",
    "# Demos\n",
    "# ==============================\n",
    "def topk_word_translations(src_tokens: List[str], tprobs: Dict[str, Dict[str, float]], k=5):\n",
    "    print(\"\\n=== Top-k word translation points (t(e|f)) ===\")\n",
    "    for s in src_tokens:\n",
    "        cands = tprobs.get(s, {})\n",
    "        ranked = sorted([(t, p) for t, p in cands.items() if t != NULL], key=lambda x: x[1], reverse=True)[:k]\n",
    "        print(f\"{s:>10} -> \", \", \".join([f'{t}:{p:.3f}' for t,p in ranked]) or \"(none)\")\n",
    "\n",
    "def translate_and_explain(zh_sent: str):\n",
    "    s_words = tok_zh_words(zh_sent)\n",
    "    en = decode_with_jumps(s_words)\n",
    "    print(\"\\nZH:\", zh_sent)\n",
    "    print(\"EN:\", \" \".join(en))\n",
    "    topk_word_translations(s_words, tprobs_s2t, k=5)\n",
    "\n",
    "for demo in [\"你好世界\", \"谢谢\", \"今天天气很好\", \"我们去学校\", \"我喜欢学习英文\", \"中国文化很有趣\",\"我爱音乐\",\"我爱你\",\"我想你\"]:\n",
    "    translate_and_explain(demo)\n",
    "\n",
    "print(\"\\nArtifacts saved in:\", RUN_DIR)\n",
    "print(\" - IBM1 checkpoints: RUN_DIR/checkpoints/ibm1_s2t_iter_XXX.json\")\n",
    "print(\" - Alignments (s2t): RUN_DIR/checkpoints/alignments_s2t.jsonl\")\n",
    "print(\" - Phrase table: RUN_DIR/phrase_table.json\")\n",
    "print(\" - Trigram LM counts: RUN_DIR/lm_trigram_counts.json\")\n",
    "print(\" - Metrics CSV: RUN_DIR/metrics.csv\")\n",
    "print(\" - Status file: RUN_DIR/status.json\")\n",
    "print(\" - Best snapshot (if any):\", BEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_mt.py\n",
    "import argparse, re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import sacrebleu\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    # simple, reproducible normalization\n",
    "    return re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "\n",
    "def tok(s: str):\n",
    "    # whitespace tokenization after normalization (good for EN-side metrics)\n",
    "    return normalize(s).split()\n",
    "\n",
    "def micro_prf(refs, hyps):\n",
    "    \"\"\"\n",
    "    Micro-averaged precision/recall/F1 at token level over the corpus.\n",
    "    Overlap counts are computed with token multiplicities (via Counter).\n",
    "    \"\"\"\n",
    "    hyp_tok_total = 0\n",
    "    ref_tok_total = 0\n",
    "    overlap_total = 0\n",
    "    exact = 0\n",
    "\n",
    "    for r, h in zip(refs, hyps):\n",
    "        r_toks, h_toks = tok(r), tok(h)\n",
    "        ref_tok_total += len(r_toks)\n",
    "        hyp_tok_total += len(h_toks)\n",
    "        rc, hc = Counter(r_toks), Counter(h_toks)\n",
    "        overlap_total += sum((rc & hc).values())\n",
    "        if normalize(r) == normalize(h):\n",
    "            exact += 1\n",
    "\n",
    "    precision = overlap_total / hyp_tok_total if hyp_tok_total else 0.0\n",
    "    recall    = overlap_total / ref_tok_total if ref_tok_total else 0.0\n",
    "    f1        = (2*precision*recall)/(precision+recall) if (precision+recall) else 0.0\n",
    "    exact_acc = exact / len(refs) if refs else 0.0\n",
    "    return precision, recall, f1, exact_acc\n",
    "\n",
    "def eval_system(name, refs, hyps):\n",
    "    # BLEU / chrF from sacrebleu\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs]).score          # default tokenize='13a'\n",
    "    chrf = sacrebleu.corpus_chrf(hyps, [refs]).score          # chrF++\n",
    "    p, r, f1, exact = micro_prf(refs, hyps)\n",
    "    return {\n",
    "        \"system\": name,\n",
    "        \"bleu\": bleu,\n",
    "        \"chrf\": chrf,\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "        \"exact_acc\": exact,\n",
    "    }\n",
    "\n",
    "def read_lines(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate MT outputs against references.\")\n",
    "    parser.add_argument(\"--ref\", required=True, help=\"Reference file (one sentence per line)\")\n",
    "    parser.add_argument(\n",
    "        \"--sys\", action=\"append\", nargs=2, metavar=(\"NAME\",\"FILE\"), required=True,\n",
    "        help=\"Repeat for each system: --sys HYBRID hybrid.txt --sys NMT nmt.txt --sys SMT smt.txt\"\n",
    "    )\n",
    "    parser.add_argument(\"--out_csv\", default=\"metrics_table.csv\", help=\"Where to save the table as CSV\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    refs = read_lines(args.ref)\n",
    "    rows = []\n",
    "\n",
    "    for name, hyp_path in args.sys:\n",
    "        hyps = read_lines(hyp_path)\n",
    "        if len(hyps) != len(refs):\n",
    "            n = min(len(refs), len(hyps))\n",
    "            print(f\"[WARN] {name}: length mismatch (refs={len(refs)} hyps={len(hyps)}); truncating to {n}\")\n",
    "            refs_eval, hyps_eval = refs[:n], hyps[:n]\n",
    "        else:\n",
    "            refs_eval, hyps_eval = refs, hyps\n",
    "        rows.append(eval_system(name, refs_eval, hyps_eval))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"system\",\"bleu\",\"chrf\",\"precision\",\"recall\",\"f1\",\"exact_acc\"])\n",
    "    df = df.round(4)\n",
    "    print(tabulate(df, headers=\"keys\", tablefmt=\"github\", showindex=False))\n",
    "    df.to_csv(args.out_csv, index=False)\n",
    "    print(f\"\\nSaved -> {args.out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
